{
  "name": "Sampling tensor decomp",
  "tagline": "Fast Approximate Orthogonal Tensor Decomposition Based on Importance Sampling",
  "body": "Sublinear Time Tensor Decomposition with Importance Sampling\r\n==================================\r\n\r\nThis repository contains experiment code for our paper, \r\n\"Sublinear Time Orthogonal Tensor Decomposition\", by Zhao Song, David P. Woodruff and Huan Zhang.\r\nIn our paper, we developed a sublinear algorithm for decomposing \r\nlarge tensors using robust tensor power method, where the\r\ncomputationally expensive tensor contractions are done by importance\r\nsampling based on the magnitude of the power iterate variable.\r\n\r\nIn our experiments we only implemented 3-rd order tensors, and \r\nsampling with or without pre-scanning variants of our algorithm.\r\nMost parts of the code is based on [1], and we only add new importance\r\nsampling based tensor contraction functions.\r\n\r\n[Spotlight Video for this paper](https://vimeo.com/192582848)\r\n[Poster for this paper](resources/poster.pdf)\r\n\r\nBuild\r\n--------------------\r\n\r\nWe require the following environment to build the code:\r\n\r\n- libfftw3 (provided by package `libfftw3-dev` on Debian based systems)\r\n- Matlab 2014b or later\r\n- GNU Compiler Collection (GCC) 4.9 or newer versions recommended\r\n- Tested on Ubuntu 16.04, should be able to run other Linux distributions or OSX\r\n\r\nYou need to modify `Makefile` to make sure Matlab includes and libraries \r\nfolders match your Matlab installation. They are defined as\r\nvariable `MAT_INC` and `MAT_LIB`:\r\n\r\n```\r\nMAT_INC = /usr/local/MATLAB/R2014b/extern/include\r\nMAT_LIB = /usr/local/MATLAB/R2014b/bin/glnxa64\r\n\r\n```\r\n\r\nTo build the program, simply run `make`. Two binaries, `fftspec`\r\nand `fft-lda` will be built. \r\nWe only need `fftspec` for experiments.\r\n\r\nDatasets\r\n---------------------------------\r\n\r\nSynthetic dense tensors with different eigengaps and noises can be generated via `fftspec`.\r\nThe following command generates a rank-100, dimension 600\\*600\\*600 tensor, with sigma=0.01 noise added.\r\nGenerated tensors will be stored under the folder \"data\".\r\n\r\n```\r\n./fftspec synth_lowrank 600 100 0.01 1\r\n```\r\n\r\nThe last parameter `1` indicates that the 100 eigenvalues decay as $\\lambda_i = 1/i$.\r\nIf you set it to `2`, then $\\lambda_i = 1/i^2$.\r\nIf you set it to `3`,then $\\lambda_i = 1 - (i-1)/k$, where k is the rank.\r\n\r\nFor the tensors from LDA (Latent Dirichlet Allocation), we provide preprocessed tensors\r\n(lzma compressed) in folder `LDA_tensor_200` for all 6 datasets we used.\r\n\r\nRun Tensor Decomposition\r\n----------------------------------\r\n\r\n4 algorithms were provided for tensor decomposition:\r\n\r\n* Naive robust tensor power method, computing exact tensor contractions\r\n* Sketching based tensor power method, based on [1]\r\n* Importance Sampling based tensor power method, without pre-scanning\r\n* Importance Sampling based tensor power method, with pre-scanning\r\n\r\nImportance Sampling with pre-scanning provides best theoretical bound where\r\nonly O(n^2) samples are needed, but it needs to scan the entire tensor first.\r\nIn practice, the without pre-scanning version (assuming the tensor has bounded\r\nslice norms, see our paper for details) works better.\r\n\r\nIn sketching or sampling based algorithms, the following parameters are \r\nneeded:\r\n\r\n* T: Number of power iterations\r\n* L: The number of starting vectors of the robust tensor power method\r\n* B: The number of sketches used in sketching, or the number of repetitions of sampling \r\n* b: The size of the sketch, or the number of indices sampled\r\n\r\nNote that there are small notation differences for the letter b between the code and paper:\r\nfor the sketching based method, the actual sketch length used is 2^b; and for \r\nimportance sampling based method, the actual total number of samples is O(nb).\r\n\r\nThe following command shows how to run tensor decomposition with `fftspec`:\r\n\r\n```\r\n./fftspec algorithm input rank L T B b output\r\n```\r\n\r\n`rank` is the number of eigenpairs to be recoveried, \r\n`algorithm` can be `slow_rbp`, `fast_rbp`, `fast_sample_rbp` and `prescan_sample_rbp`, which\r\ncorresponds to the 4 algorithms metioned above, respectively.\r\nFor naive tensor power method `slow_rbp`, parameter B and b are not needed.\r\n\r\nThe following example shows how to generate a synthetic tensor, and run different methods\r\nto compare their running time and recovery accuracy:\r\n\r\n```\r\n# generate a 800x800x800, rank-100 tensor (takes a while...)\r\n./fftspec synth_lowrank 800 100 0.01 2\r\n# Run sketching based tensor power method\r\n./fftspec fast_rbp data/tensor_dim_800_rank_100_noise_0.01_decaymethod_2.dat 1 50 30 30 16 output_dim_800_rank_100_noise_0.01_decaymethod_2_fastrbp_50_30_30_16_rank1.dat\r\n# Run sampling based tensor power method, without prescanning\r\n./fftspec fast_sample_rbp data/tensor_dim_800_rank_100_noise_0.01_decaymethod_2.dat 1 50 30 30 10 output_dim_800_rank_100_noise_0.01_decaymethod_2_samplerbp_50_30_30_10_rank1.dat\r\n# Run sampling based tensor power method, with prescanning\r\n./fftspec prescan_sample_rbp data/tensor_dim_800_rank_100_noise_0.01_decaymethod_2.dat 1 50 30 30 10 output_dim_800_rank_100_noise_0.01_decaymethod_2_presamplerbp_50_30_30_10_rank1.dat\r\n# Run naive tensor power method (SLOW!)\r\n./fftspec slow_rbp data/tensor_dim_800_rank_100_noise_0.01_decaymethod_2.dat 1 50 30 output_dim_800_rank_100_noise_0.01_decaymethod_2_slowrbp_50_30_rank1.dat\r\n# \r\n```\r\n\r\nWe want the residual norm to be small, while keeping the reported CPU time as\r\nshort as possible.  Fixing T and L, you can try different B and b and see how\r\nrunning time and residual change.  Generally, using a smaller B and b makes the\r\nalgorithm run faster, but the residual is likely to increase and divergence may\r\noccur when B or b is too small.  \r\n\r\nFor the example above here is the expected\r\noutputs. If you got numbers quite different from these you probably hit a bug.\r\n\r\n* For sketching based tensor power method:\r\n```\r\n# [STAT]: prep_cpu_time=12.654545\r\n# [STAT]: prep_wall_time=12.654588\r\n# [STAT]: cpu_time=69.057999\r\n# [STAT]: wall_time=69.058114\r\n# [STAT]: residue=0.089962\r\n# [STAT]: fnorm=1.010029\r\n```\r\n\r\n* For importance sampling based tensor power method (without pre-scanning, no preprocessing time):\r\n```\r\n# [STAT]: cpu_time=9.078225\r\n# [STAT]: wall_time=9.078256\r\n# [STAT]: residue=0.086472\r\n# [STAT]: fnorm=1.010029\r\n\r\n```\r\n\r\n* For importance sampling based tensor power method (with pre-scanning):\r\n```\r\n# [STAT]: prep_cpu_time=0.659961\r\n# [STAT]: prep_wall_time=0.659971\r\n# [STAT]: cpu_time=10.123327\r\n# [STAT]: wall_time=10.123355\r\n# [STAT]: residue=0.086454\r\n# [STAT]: fnorm=1.010029\r\n\r\n```\r\n\r\n* For naive tensor power method:\r\n```\r\nToo long, don't run!\r\n```\r\n\r\nResults\r\n----------------------------------\r\n\r\nWe have included a large range of results in the `results` folder, for both sketching and importance sampling\r\nbased parameters with different B and b, and different eigenvalue decay rates. \r\n\r\nContact information\r\n----------------------------------\r\n\r\nIf your have any questions or comments, please open an issue on Github,\r\nor send an email to ecezhang@ucdavis.edu. We appreciate your feedback.\r\n\r\nReferences\r\n----------------------------------\r\n\r\n[1] Yining Wang, Hsiao-Yu Tung, Alex J Smola, and Anima Anandkumar. Fast and\r\nguaranteed tensor decomposition via sketching. In NIPS, pages 991-999, 2015.\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}